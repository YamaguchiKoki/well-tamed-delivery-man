import asyncio
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Any

from ..models import ExecutorResult, UnifiedItem, SourceType

logger = logging.getLogger(__name__)

def _create_unified_item(tweet: Dict[str, Any], collected_at: datetime) -> UnifiedItem:
    """Tweetãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›"""

    content_id = f"twitter_{hashlib.md5(tweet['text'].encode()).hexdigest()[:8]}"

    # Tweetã®å ´åˆã€textãŒçŸ­ã„ã®ã§ãã®ã¾ã¾è¦ç´„ã¨ã—ã¦ä½¿ç”¨
    summary = tweet['text'][:200] + "..." if len(tweet['text']) > 200 else tweet['text']

    return UnifiedItem(
        id=content_id,
        title=f"Tweet by {tweet['author']}",
        summary=summary,
        url=tweet['url'],
        source='twitter',
        source_type=SourceType.SOCIAL,
        published_at=datetime.fromisoformat(tweet['timestamp'].replace('Z', '+00:00') if 'Z' in tweet['timestamp'] else tweet['timestamp']),
        collected_at=collected_at
    )

async def twitter_fetcher(config: Dict[str, Any]) -> ExecutorResult:
    """TwitteræŠ•ç¨¿å–å¾—ï¼ˆLevel 1ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œç‰ˆï¼‰"""

    accounts = config.get("accounts", ["@openai", "@huggingface"])
    keywords = config.get("keywords", ["AI", "machine learning"])
    max_tweets = config.get("max_tweets", 50)
    use_mock = config.get("use_mock", True)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨

    logger.info(f"Fetching Twitter data: {len(accounts)} accounts, {len(keywords)} keywords")

    collected_at = datetime.now()

    if use_mock:
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        await asyncio.sleep(0.5)

        # ã‚ˆã‚Šãƒªã‚¢ãƒ«ãªTwitterã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        sample_tweets = [
            {
                "text": "Excited to announce our new AI research breakthrough in multimodal understanding! ğŸš€ #AI #MachineLearning",
                "author": "@openai",
                "url": "https://twitter.com/openai/status/sample1",
                "keyword": "AI",
                "timestamp": (datetime.now() - timedelta(hours=1)).isoformat(),
                "likes": 2847,
                "retweets": 892
            },
            {
                "text": "New Transformers library update with improved performance and memory efficiency! Check it out ğŸ¤—",
                "author": "@huggingface",
                "url": "https://twitter.com/huggingface/status/sample2",
                "keyword": "machine learning",
                "timestamp": (datetime.now() - timedelta(hours=3)).isoformat(),
                "likes": 1567,
                "retweets": 445
            },
            {
                "text": "The future of AI is multimodal. Combining vision, language, and reasoning capabilities will unlock new possibilities.",
                "author": "@openai",
                "url": "https://twitter.com/openai/status/sample3",
                "keyword": "AI",
                "timestamp": (datetime.now() - timedelta(hours=8)).isoformat(),
                "likes": 3421,
                "retweets": 1204
            },
            {
                "text": "Open source AI models are democratizing access to cutting-edge technology. What are you building with them?",
                "author": "@huggingface",
                "url": "https://twitter.com/huggingface/status/sample4",
                "keyword": "machine learning",
                "timestamp": (datetime.now() - timedelta(hours=12)).isoformat(),
                "likes": 987,
                "retweets": 234
            }
        ]

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        tweets = []
        for keyword in keywords:
            keyword_tweets = [t for t in sample_tweets if keyword.lower() in t['text'].lower()]
            tweets.extend(keyword_tweets[:max_tweets // len(keywords)])

        # ä¸è¶³åˆ†ã¯æ±ç”¨çš„ãªãƒ„ã‚¤ãƒ¼ãƒˆã§è£œå®Œ
        remaining = min(max_tweets - len(tweets), 10)  # æœ€å¤§10å€‹ã¾ã§
        for i in range(remaining):
            account = accounts[i % len(accounts)]
            keyword = keywords[i % len(keywords)]
            tweets.append({
                "text": f"Sample tweet about {keyword} from {account}. This is auto-generated content for testing purposes.",
                "author": account,
                "url": f"https://twitter.com{account}/status/gen{i}",
                "keyword": keyword,
                "timestamp": (datetime.now() - timedelta(hours=i+1)).isoformat(),
                "likes": (i + 1) * 15,
                "retweets": (i + 1) * 5
            })

    else:
        # TODO: å®Ÿéš›ã®Twitter APIå®Ÿè£…
        logger.info("Twitter API not implemented yet, using fallback data")
        tweets = [
            {
                "text": f"Fallback tweet about {keyword}",
                "author": account,
                "url": f"https://twitter.com{account}/fallback{i}",
                "keyword": keyword,
                "timestamp": (datetime.now() - timedelta(hours=i)).isoformat(),
                "likes": i * 20,
                "retweets": i * 6
            }
            for i, (account, keyword) in enumerate(zip(accounts, keywords), 1)
        ]

    # çµ±ä¸€å½¢å¼ã«å¤‰æ›
    unified_items = []
    for tweet in tweets:
        try:
            unified_item = _create_unified_item(tweet, collected_at)
            unified_items.append(unified_item)
        except Exception as e:
            logger.warning(f"Failed to convert Tweet to unified format: {e}")
            continue

    return {
        "unified_items": [item.model_dump() for item in unified_items],
        "count": len(unified_items),
        "source": "twitter",
        "source_type": "social",
        "collected_at": collected_at.isoformat()
    }
